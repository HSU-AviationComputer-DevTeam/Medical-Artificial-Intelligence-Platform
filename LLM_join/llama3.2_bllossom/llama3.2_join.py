import os
import logging
import google.generativeai as genai
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.llms import HuggingFacePipeline
import traceback
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE" #openMP ì¶©ëŒë°©ì§€


# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- 1. ì„¤ì • ---
GOOGLE_API_KEY = input("Google API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ") 
if not GOOGLE_API_KEY.strip():
    logger.error("ìœ íš¨í•œ Google API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    exit()

VECTOR_STORE_PATH = "C:\code\medical_llama3.2"  # index.faiss, index.pkl ìˆëŠ” ê²½ë¡œ ì„¤ì •
OLLAMA_MODEL_NAME = "llama-3.2-Korean-Bllossom-AICA-5B"   # LLM ì—°ê²°ì— ì‚¬ìš©í•  ëª¨ë¸

# ğŸš¨ ì‚¬ìš©ìê°€ ìš”ì²­í•œ íŠ¹ì • ì‹¤í—˜ì  ëª¨ë¸. í˜„ì¬ ì§€ì›ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
EMBEDDING_MODEL = "models/gemini-embedding-exp-03-07" 

# --- 2. Gemini ì„ë² ë”© í´ë˜ìŠ¤ ì •ì˜ (ìš”ì²­ ëª¨ë¸ ì‚¬ìš©) ---
class CustomGeminiEmbeddings:
    """Google Gemini íŠ¹ì • ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ì»¤ìŠ¤í…€ í´ë˜ìŠ¤."""
    def __init__(self, model):
        self.model = model

    def embed_documents(self, texts):
        """ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤."""
        if isinstance(texts, str): texts = [texts]
        logger.info(f"'{self.model}'ë¡œ ë¬¸ì„œ ì„ë² ë”© ì¤‘...")
        try:
            # google-generativeai ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§ì ‘ í˜¸ì¶œ
            result = genai.embed_content(
                model=self.model,
                content=texts,
                task_type="RETRIEVAL_DOCUMENT" # ëŒ€ë¬¸ì ì‚¬ìš© ì‹œë„
            )
            return result['embedding']
        except Exception as e:
            logger.error(f"'{self.model}' ë¬¸ì„œ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            logger.warning("ì˜¤ë¥˜ ë°œìƒ! ì´ ëª¨ë¸ì€ ë” ì´ìƒ ì§€ì›ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            raise # ì˜¤ë¥˜ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ ì¤‘ë‹¨

    def embed_query(self, text):
        """ë‹¨ì¼ ì¿¼ë¦¬ë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤."""
        logger.info(f"'{self.model}'ë¡œ ì¿¼ë¦¬ ì„ë² ë”© ì¤‘...")
        try:
            result = genai.embed_content(
                model=self.model,
                content=[text],
                task_type="RETRIEVAL_QUERY" # ëŒ€ë¬¸ì ì‚¬ìš© ì‹œë„
            )
            return result['embedding'][0]
        except Exception as e:
            logger.error(f"'{self.model}' ì¿¼ë¦¬ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            logger.warning("ì˜¤ë¥˜ ë°œìƒ! ì´ ëª¨ë¸ì€ ë” ì´ìƒ ì§€ì›ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            raise # ì˜¤ë¥˜ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ ì¤‘ë‹¨

    def __call__(self, text):
        return self.embed_query(text)

# --- 3. ì„ë² ë”© ê°ì²´ ë° ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ---
try:
    # Google API êµ¬ì„±
    genai.configure(api_key=GOOGLE_API_KEY.strip())
    
    # ì»¤ìŠ¤í…€ ì„ë² ë”© ê°ì²´ ìƒì„±
    embeddings = CustomGeminiEmbeddings(model=EMBEDDING_MODEL)
    logger.info(f"'{EMBEDDING_MODEL}' ì»¤ìŠ¤í…€ ì„ë² ë”© ê°ì²´ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

except Exception as e:
    logger.error(f"Google API êµ¬ì„± ë˜ëŠ” ì„ë² ë”© ê°ì²´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    logger.error(traceback.format_exc())
    exit()

try:
    if os.path.exists(VECTOR_STORE_PATH) and \
       os.path.exists(os.path.join(VECTOR_STORE_PATH, "index.faiss")):
        logger.info(f"{VECTOR_STORE_PATH}ì—ì„œ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
        vectorstore = FAISS.load_local(
            folder_path=VECTOR_STORE_PATH, 
            embeddings=embeddings, # ì»¤ìŠ¤í…€ ì„ë² ë”© ê°ì²´ ì „ë‹¬
            allow_dangerous_deserialization=True
        )
        logger.info("FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    else:
        logger.error(f"ì˜¤ë¥˜: ë²¡í„° ìŠ¤í† ì–´ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {VECTOR_STORE_PATH}")
        exit()
except Exception as e:
    logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    logger.error("ğŸš¨ ì‚¬ìš©ëœ ì„ë² ë”© ëª¨ë¸ê³¼ ë²¡í„° ìŠ¤í† ì–´ê°€ í˜¸í™˜ë˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    logger.error(traceback.format_exc())
    exit()

# --- 4. LLM ë¡œë“œ (HuggingFace Llama 3.2 Bllossom) ---
try:
    logger.info("Hugging Face ëª¨ë¸ì„ ë¡œì»¬ì—ì„œ ë¡œë“œ ì¤‘...")

    model_name_or_path = "Bllossom/llama-3.2-Korean-Bllossom-AICA-5B"

    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name_or_path,
        device_map="auto",          # GPU ì‚¬ìš©
        torch_dtype="auto"
    )

    hf_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=1024,
        temperature=0.7,
        do_sample=True,
        repetition_penalty=1.1
    )

    llm = HuggingFacePipeline(pipeline=hf_pipeline)
    logger.info(f"HuggingFace '{model_name_or_path}' LLMì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

except Exception as e:
    logger.error(f"HuggingFace LLM ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    logger.error(traceback.format_exc())
    exit()

# --- 5. QA ì²´ì¸ ìƒì„± (í”„ë¡¬í”„íŠ¸ ì»¤ìŠ¤í„°ë§ˆì´ì§• í¬í•¨) ---

# í•œêµ­ì–´ ì§€ì‹œê°€ í¬í•¨ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
ë‹¹ì‹ ì€ ì˜ë£Œ ì •ë³´ë¥¼ ë¶„ì„í•˜ëŠ” í•œêµ­ì–´(korea) AIì…ë‹ˆë‹¤.
ë‹¤ìŒ ë¬¸ë§¥(context)ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— í•œêµ­ì–´(korea)ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì˜ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ë§ˆì„¸ìš”.

ë¬¸ë§¥:
{context}

ì§ˆë¬¸:
{question}
""",
)

# ì²´ì¸ ìƒì„± ì‹œ prompt ëª…ì‹œ
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": custom_prompt}
)
logger.info("RetrievalQA ì²´ì¸ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

# --- 6. ì§ˆë¬¸ ë° ë‹µë³€ ---
def ask_question(query):
    print(f"\n========================================")
    print(f"ğŸ’¬ ì§ˆë¬¸: {query}")
    print(f"========================================")
    try:
        result = qa_chain.invoke({"query": query})
        print("\nğŸ’¡ ë‹µë³€:")
        print(result['result'])
        print("\nğŸ“š ê·¼ê±° ë¬¸ì„œ:")
        for i, doc in enumerate(result['source_documents']):
            doc_type = doc.metadata.get('document_type', 'N/A')
            p_name = doc.metadata.get('name', 'N/A')
            p_id = doc.metadata.get('patient_id', 'N/A')
            print(f"  [{i+1}] ìœ í˜•:{doc_type}, í™˜ì:{p_name}({p_id}) | ë‚´ìš©: {doc.page_content[:300]}...")
    except Exception as e:
        logger.error(f"ì§ˆì˜ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(traceback.format_exc())

# --- ì‹¤í–‰ ---
if __name__ == "__main__":
    while True:
        user_query = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'q' ì…ë ¥): ")
        if user_query.lower() == 'q':
            break
        if user_query.strip():
            ask_question(user_query)
        else:
            print("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")