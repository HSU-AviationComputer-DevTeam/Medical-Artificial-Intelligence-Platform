"""Medical tools for patient search, document generation, and analysis."""

import asyncio
import json
import os
import time
from datetime import datetime
from typing import Any, Dict, List, Optional

import aiohttp
import google.generativeai as genai
import numpy as np
import pandas as pd
import requests
from langchain.tools import BaseTool
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from pydantic import BaseModel, Field

from models import (
    DepartmentType,
    DrugInteraction,
    MedicalAnalysis,
    MedicalRecord,
    Patient,
    PatientSearchQuery,
    PatientSearchResult,
    SOAPNote,
    UrgencyLevel,
)


class PatientSearchTool(BaseTool):
    """í™˜ì ê²€ìƒ‰ ë„êµ¬"""
    
    name: str = "patient_search"
    description: str = "í™˜ì ID, ì´ë¦„, ì¦ìƒìœ¼ë¡œ í™˜ìë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."
    data_path: str = Field(default="../../../../../VectorStore2/medical_data")
    patients_data: Dict[str, List[Dict]] = Field(default_factory=dict)
    
    def __init__(self, data_path: str = "../../../../../VectorStore2/medical_data", **kwargs):
        super().__init__(**kwargs)
        self.data_path = data_path
        self.patients_data = self._load_patient_data()
    
    def _load_patient_data(self) -> Dict[str, List[Dict]]:
        """í™˜ì ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        data = {}
        
        # VectorStore2/medical_dataì˜ JSON íŒŒì¼ ë§¤í•‘
        file_mappings = {
            "cardiology_patients.json": "ì‹¬ì¥ë‚´ê³¼",
            "emergency_patients.json": "ì‘ê¸‰ì˜í•™ê³¼", 
            "internal_medicine_patients.json": "ë‚´ê³¼",
            "neurology_patients.json": "ì‹ ê²½ê³¼",
            "surgery_patients.json": "ì™¸ê³¼"
        }
        
        print(f"Loading patient data from: {self.data_path}")
        
        for filename, department in file_mappings.items():
            file_path = os.path.join(self.data_path, filename)
            print(f"Checking file: {file_path}")
            
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        patient_list = json.load(f)
                        dept_data = []
                        
                        # JSON íŒŒì¼ì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê° í™˜ìì— department ì¶”ê°€
                        if isinstance(patient_list, list):
                            for patient in patient_list:
                                patient['department'] = department
                                dept_data.append(patient)
                        else:
                            # ë‹¨ì¼ í™˜ì ê°ì²´ì¸ ê²½ìš°
                            patient_list['department'] = department
                            dept_data.append(patient_list)
                        
                        data[department] = dept_data
                        print(f"âœ… Loaded {filename}: {len(dept_data)} patients")
                        
                except Exception as e:
                    print(f"âŒ Error loading {filename}: {e}")
            else:
                print(f"âŒ File not found: {file_path}")
        
        print(f"\nğŸ“Š ì´ ë¡œë“œëœ ë°ì´í„°: {len(data)} ë¶€ì„œ")
        total_patients = 0
        for dept, patients in data.items():
            patient_count = len(patients)
            total_patients += patient_count
            print(f"  ğŸ¥ {dept}: {patient_count}ëª…")
        print(f"  ğŸ‘¥ ì „ì²´ í™˜ì ìˆ˜: {total_patients}ëª…\n")
        
        return data
    
    def _run(self, query: str, query_type: str = "auto", max_results: int = 10) -> str:
        """í™˜ì ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        start_time = time.time()
        results = []
        
        query_lower = query.lower()
        
        # ìë™ ì¿¼ë¦¬ íƒ€ì… ê°ì§€
        if query_type == "auto":
            query_type = self._detect_query_type(query_lower)
        
        # ì¿¼ë¦¬ì—ì„œ ê²€ìƒ‰ íƒ€ì…ê³¼ ê°’ì„ íŒŒì‹±
        if ":" in query:
            parts = query.split(":", 1)
            if len(parts) == 2:
                search_type = parts[0].strip()
                search_value = parts[1].strip()
                
                if "ì´ë¦„" in search_type or "name" in search_type.lower():
                    query_type = "name"
                    query_lower = search_value.lower()
                elif "ì¦ìƒ" in search_type or "symptom" in search_type.lower():
                    query_type = "symptom"
                    query_lower = search_value.lower()
                elif "ì§„ë‹¨" in search_type or "diagnosis" in search_type.lower():
                    query_type = "diagnosis"
                    query_lower = search_value.lower()
        
        print(f"ğŸ” ê²€ìƒ‰ ì‹¤í–‰: '{query}' (íƒ€ì…: {query_type})")
        
        # ë³µí•© ê²€ìƒ‰: ì—¬ëŸ¬ í•„ë“œì—ì„œ ë™ì‹œ ê²€ìƒ‰
        for dept, patients in self.patients_data.items():
            for patient in patients:
                match_found = False
                match_fields = []
                
                # ì´ë¦„ ê²€ìƒ‰
                if query_type in ["name", "all"] and query_lower in patient.get('name', '').lower():
                    match_found = True
                    match_fields.append("ì´ë¦„")
                
                # ID ê²€ìƒ‰
                if query_type in ["id", "all"] and query_lower in patient.get('id', '').lower():
                    match_found = True
                    match_fields.append("ID")
                
                # ì§„ë‹¨ ê²€ìƒ‰ (ì˜ë£Œ ìš©ì–´ ë§¤í•‘ í¬í•¨)
                if query_type in ["diagnosis", "all"]:
                    # diagnoses ë°°ì—´ì—ì„œ ê²€ìƒ‰
                    diagnoses = patient.get('diagnoses', [])
                    if isinstance(diagnoses, list):
                        for diag in diagnoses:
                            if isinstance(diag, dict):
                                diag_name = diag.get('name', '').lower()
                                if self._matches_medical_term(query_lower, diag_name):
                                    match_found = True
                                    match_fields.append("ì§„ë‹¨")
                                    break
                    
                    # ë‹¨ì¼ diagnosis í•„ë“œë„ í™•ì¸ (í•˜ìœ„ í˜¸í™˜ì„±)
                    diagnosis = patient.get('diagnosis', '').lower()
                    if diagnosis and self._matches_medical_term(query_lower, diagnosis):
                        match_found = True
                        match_fields.append("ì§„ë‹¨")
                
                # ì¦ìƒ ê²€ìƒ‰
                if query_type in ["symptom", "all"]:
                    symptoms = patient.get('symptoms', [])
                    if isinstance(symptoms, list):
                        for symptom in symptoms:
                            if self._matches_medical_term(query_lower, symptom.lower()):
                                match_found = True
                                match_fields.append("ì¦ìƒ")
                                break
                    elif isinstance(symptoms, str) and self._matches_medical_term(query_lower, symptoms.lower()):
                        match_found = True
                        match_fields.append("ì¦ìƒ")
                
                if match_found:
                    patient_result = patient.copy()
                    patient_result['match_fields'] = match_fields
                    results.append(patient_result)
        
        search_time = time.time() - start_time
        
        # ê²°ê³¼ ì œí•œ
        results = results[:max_results]
        
        print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ëª… ë°œê²¬ ({search_time:.3f}ì´ˆ)")
        
        return json.dumps({
            "patients": results,
            "total_count": len(results),
            "search_time": search_time,
            "query": query,
            "query_type": query_type,
            "search_performed": True
        }, ensure_ascii=False, indent=2)
    
    def _detect_query_type(self, query: str) -> str:
        """ì¿¼ë¦¬ íƒ€ì…ì„ ìë™ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤."""
        # ì˜ë£Œ ìš©ì–´ë‚˜ ì¦ìƒ ê´€ë ¨ í‚¤ì›Œë“œ ê°ì§€
        medical_terms = ['í™˜ì', 'ë³‘', 'ì§ˆí™˜', 'ì¦ìƒ', 'í†µì¦', 'ì—´', 'ê¸°ì¹¨', 'ë‘í†µ', 'ë³µí†µ', 'ë‹¹ë‡¨', 'ê³ í˜ˆì••', 'ì•”', 'ì‹¬ì¥', 'í', 'ê°„', 'ì‹ ì¥']
        
        if any(term in query for term in medical_terms):
            if 'í™˜ì' in query:
                return "diagnosis"  # "ë‹¹ë‡¨ë³‘í™˜ì" -> ì§„ë‹¨ëª…ìœ¼ë¡œ ê²€ìƒ‰
            else:
                return "symptom"
        
        # ID íŒ¨í„´ ê°ì§€ (P001, H123 ë“±)
        if len(query) <= 10 and any(char.isdigit() for char in query):
            return "id"
        
        # ê¸°ë³¸ê°’ì€ ì „ì²´ ê²€ìƒ‰
        return "all"
    
    def _matches_medical_term(self, search_term: str, target_text: str) -> bool:
        """ì˜ë£Œ ìš©ì–´ ë§¤ì¹­ (ìœ ì‚¬ì–´ ë° ë¶€ë¶„ ë§¤ì¹­ í¬í•¨)"""
        # ì§ì ‘ ë§¤ì¹­
        if search_term in target_text:
            return True
        
        # ì˜ë£Œ ìš©ì–´ ë§¤í•‘
        medical_mappings = {
            'ë‹¹ë‡¨': ['diabetes', 'ë‹¹ë‡¨ë³‘', 'dm', 'í˜ˆë‹¹'],
            'ê³ í˜ˆì••': ['hypertension', 'í˜ˆì••', 'htn', 'ê³ í˜ˆì••ì¦'],
            'ì‹¬ì¥': ['cardiac', 'heart', 'ì‹¬ê·¼', 'ê´€ìƒë™ë§¥'],
            'ì•”': ['cancer', 'ì¢…ì–‘', 'tumor', 'carcinoma'],
            'ì—´': ['fever', 'ë°œì—´', 'ì²´ì˜¨'],
            'í†µì¦': ['pain', 'ì•„í””', 'ache'],
            'ê¸°ì¹¨': ['cough', 'í•´ìˆ˜'],
            'í˜¸í¡': ['breathing', 'respiratory', 'ìˆ¨', 'í˜¸í¡ê³¤ë€']
        }
        
        # ë§¤í•‘ëœ ìš©ì–´ë¡œ ê²€ìƒ‰
        for key, synonyms in medical_mappings.items():
            if key in search_term:
                for synonym in synonyms:
                    if synonym in target_text:
                        return True
        
        return False


class GeminiEmbeddings:
    """Gemini ì„ë² ë”© í´ë˜ìŠ¤ - LangChain í˜¸í™˜"""
    
    def __init__(self, api_key: str):
        genai.configure(api_key=api_key)
        self.api_key = api_key
    
    def __call__(self, text):
        """LangChain í˜¸í™˜ì„ ìœ„í•œ callable ë©”ì„œë“œ"""
        return self.embed_query(text)
    
    def embed_documents(self, texts):
        """ë¬¸ì„œ ì„ë² ë”© (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)"""
        raise NotImplementedError("ì´ ë©”ì„œë“œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    def embed_query(self, text):
        """ì¿¼ë¦¬ ì„ë² ë”©"""
        try:
            # ì›ë˜ ë…¸íŠ¸ë¶ì—ì„œ ì‚¬ìš©í•œ ì •í™•í•œ ëª¨ë¸ ì‚¬ìš© (3072ì°¨ì›)
            print(f"ğŸ” Gemini ì„ë² ë”© ì‹¤í–‰: gemini-embedding-exp-03-07")
            result = genai.embed_content(
                model="models/gemini-embedding-exp-03-07",
                content=text,
                task_type="retrieval_query"  # ë…¸íŠ¸ë¶ì—ì„œëŠ” retrieval_documentì˜€ì§€ë§Œ ì¿¼ë¦¬ì—ëŠ” retrieval_query
            )
            embedding = result['embedding']
            print(f"âœ… ì„ë² ë”© ì„±ê³µ! ì°¨ì›: {len(embedding)}")
            return embedding
        except Exception as e:
            print(f"âŒ Gemini ì„ë² ë”© ì˜¤ë¥˜ (gemini-embedding-exp-03-07): {e}")
            # text-embedding-004ë¡œ fallback ì‹œë„ 
            try:
                print("ğŸ”„ text-embedding-004 ëª¨ë¸ë¡œ fallback...")
                result = genai.embed_content(
                    model="models/text-embedding-004",
                    content=text,
                    task_type="retrieval_query"
                )
                embedding = result['embedding']
                print(f"âš ï¸ Fallback ì„±ê³µ! ì°¨ì›: {len(embedding)} (ì°¨ì› ë¶ˆì¼ì¹˜ ê°€ëŠ¥)")
                return embedding
            except Exception as e2:
                print(f"âŒ ëª¨ë“  Gemini ëª¨ë¸ ì‹¤íŒ¨: {e2}")
                raise


class VectorSearchTool(BaseTool):
    """ë²¡í„° ê²€ìƒ‰ ë„êµ¬ - ìœ ì‚¬ ì¦ìƒ í™˜ì ê²€ìƒ‰"""
    
    name: str = "vector_search"
    description: str = "ì¦ìƒì„ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬í•œ í™˜ìë¥¼ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤."
    openai_api_key: str = Field(default="")
    gemini_api_key: str = Field(default="")
    vectorstore: Optional[str] = Field(default=None)
    
    def __init__(self, openai_api_key: str = "", gemini_api_key: str = "", **kwargs):
        super().__init__(**kwargs)
        self.openai_api_key = openai_api_key
        self.gemini_api_key = gemini_api_key
        self.vectorstore = None
        self._initialize_vectorstore()
    
    def _initialize_vectorstore(self):
        """ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        try:
            # ì‹¤ì œ ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ ì‚¬ìš©
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.join(current_dir, "../../../../..")
            vector_path = os.path.join(project_root, "VectorStore2", "vector_stores", "medical_vector_store")
            vector_path = os.path.abspath(vector_path)
            
            print(f"ğŸ” ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ: {vector_path}")
            
            # FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„
            try:
                from langchain_community.vectorstores import FAISS

                # ê°•ì œë¡œ Gemini ìš°ì„  ì‹œë„ (3072ì°¨ì› ë²¡í„°ìŠ¤í† ì–´ìš©)
                if self.gemini_api_key:
                    print("ğŸŒŸ Gemini ì„ë² ë”©ìœ¼ë¡œ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„ (3072ì°¨ì›)")
                    try:
                        embeddings = GeminiEmbeddings(api_key=self.gemini_api_key)
                        # FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
                        self.vectorstore = FAISS.load_local(vector_path, embeddings, allow_dangerous_deserialization=True)
                        print("âœ… FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì„±ê³µ! (Gemini 3072ì°¨ì›)")
                        return
                    except Exception as gemini_error:
                        print(f"âŒ Gemini ë¡œë“œ ì‹¤íŒ¨: {gemini_error}")
                        print("ğŸ”„ OpenAIë¡œ fallback ì‹œë„...")
                
                # OpenAI API í‚¤ê°€ ìˆìœ¼ë©´ OpenAI ì„ë² ë”© ì‚¬ìš© (1536ì°¨ì›) - Fallback
                if self.openai_api_key:
                    print("ğŸ”‘ OpenAI ì„ë² ë”©ìœ¼ë¡œ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„")
                    from langchain_openai import OpenAIEmbeddings
                    embeddings = OpenAIEmbeddings(openai_api_key=self.openai_api_key)
                    # FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
                    self.vectorstore = FAISS.load_local(vector_path, embeddings, allow_dangerous_deserialization=True)
                    print("âœ… FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì„±ê³µ! (OpenAI 1536ì°¨ì›)")
                    return
                else:
                    print("âš ï¸ API í‚¤ê°€ ì—†ì–´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"âŒ FAISS ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # ëŒ€ì•ˆ: ChromaDB ì‹œë„
            try:
                # ChromaDB ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„
                chroma_path = os.path.join(vector_path, "chroma_db")
                if os.path.exists(chroma_path):
                    print(f"ğŸ” ChromaDB ì‹œë„: {chroma_path}")
                    self.vectorstore = "chroma_available"
                    print("âœ… ChromaDB ë²¡í„°ìŠ¤í† ì–´ ë°œê²¬!")
                    return
            except Exception as e:
                print(f"âŒ ChromaDB ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # íŒŒì¼ í™•ì¸
            index_path = os.path.join(vector_path, "index.faiss")
            pkl_path = os.path.join(vector_path, "index.pkl")
            
            print(f"ğŸ“ FAISS íŒŒì¼ í™•ì¸:")
            print(f"  - index.faiss: {os.path.exists(index_path)}")
            print(f"  - index.pkl: {os.path.exists(pkl_path)}")
            
            if os.path.exists(index_path) and os.path.exists(pkl_path):
                self.vectorstore = "faiss_files_found"
                print("âœ… ë²¡í„°ìŠ¤í† ì–´ íŒŒì¼ë“¤ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
            else:
                print("âŒ ë²¡í„°ìŠ¤í† ì–´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ Vector store initialization error: {e}")
    
    def _run(self, symptoms: str, k: int = 5) -> str:
        """ìœ ì‚¬ ì¦ìƒ í™˜ìë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        if not self.vectorstore:
            return json.dumps({"error": "Vector store not available"}, ensure_ascii=False)
        
        try:
            # ì‹¤ì œ FAISS ë²¡í„°ìŠ¤í† ì–´ê°€ ë¡œë“œëœ ê²½ìš°
            if hasattr(self.vectorstore, 'similarity_search_with_score'):
                print(f"ğŸ” ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰: '{symptoms}'")
                
                # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
                docs_with_scores = self.vectorstore.similarity_search_with_score(symptoms, k=k)
                
                results = []
                for doc, score in docs_with_scores:
                    results.append({
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "similarity_score": float(1 - score)  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                    })
                
                print(f"âœ… ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ ë°œê²¬")
                
                return json.dumps({
                    "similar_cases": results,
                    "query": symptoms,
                    "total_found": len(results),
                    "source": "FAISS VectorStore",
                    "search_type": "semantic_vector_search"
                }, ensure_ascii=False, indent=2)
            
            # ë²¡í„°ìŠ¤í† ì–´ íŒŒì¼ì´ ìˆì§€ë§Œ ì°¨ì› ë¶ˆì¼ì¹˜ ë“±ìœ¼ë¡œ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°
            elif self.vectorstore in ["faiss_files_found", "chroma_available"] or not hasattr(self.vectorstore, 'similarity_search_with_score'):
                print(f"ğŸ§  ë²¡í„° ê²€ìƒ‰ ëŒ€ì‹  í–¥ìƒëœ ì˜ë¯¸ì  í‚¤ì›Œë“œ ê²€ìƒ‰ ì‚¬ìš©")
                
                # í–¥ìƒëœ ì˜ë¯¸ì  ê²€ìƒ‰ - ë‹¤ì–‘í•œ í‚¤ì›Œë“œë¡œ í™•ì¥
                expanded_queries = self._expand_medical_query(symptoms)
                print(f"ğŸ” í™•ì¥ëœ ê²€ìƒ‰ì–´: {expanded_queries}")
                
                all_results = []
                for query in expanded_queries:
                    # ì¦ìƒ ê¸°ë°˜ ê²€ìƒ‰
                    symptom_results = self._search_json_data(query, "symptom")
                    # ì§„ë‹¨ ê¸°ë°˜ ê²€ìƒ‰  
                    diagnosis_results = self._search_json_data(query, "diagnosis")
                    all_results.extend(symptom_results + diagnosis_results)
                
                # ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ì„± ìŠ¤ì½”ì–´ ê³„ì‚°
                unique_results = self._deduplicate_and_score(all_results, symptoms)
                
                return json.dumps({
                    "similar_cases": unique_results[:k],
                    "query": symptoms,
                    "total_found": len(unique_results),
                    "source": "í–¥ìƒëœ ì˜ë¯¸ì  í‚¤ì›Œë“œ ê²€ìƒ‰",
                    "search_type": "enhanced_semantic_keyword_search",
                    "expanded_queries": expanded_queries
                }, ensure_ascii=False, indent=2)
            
            else:
                return json.dumps({
                    "error": "Vector store not properly initialized",
                    "vectorstore_status": str(self.vectorstore)
                }, ensure_ascii=False)
            
        except Exception as e:
            print(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            return json.dumps({"error": f"Vector search failed: {str(e)}"}, ensure_ascii=False)
    
    def _expand_medical_query(self, query: str) -> List[str]:
        """ì˜ë£Œ ì¿¼ë¦¬ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ í™•ì¥í•©ë‹ˆë‹¤."""
        expanded = [query]  # ì›ë³¸ í¬í•¨
        
        # ì˜ë£Œ ìš©ì–´ í™•ì¥ ë§¤í•‘
        medical_expansions = {
            'ë‹¹ë‡¨ë³‘': ['diabetes', 'í˜ˆë‹¹', 'ì¸ìŠë¦°', 'ë‹¹ë‡¨', 'dm', 'ì œ2í˜• ë‹¹ë‡¨ë³‘', 'ì œ1í˜• ë‹¹ë‡¨ë³‘'],
            'ê³ í˜ˆì••': ['hypertension', 'í˜ˆì••', 'htn', 'ê³ í˜ˆì••ì¦', 'ìˆ˜ì¶•ê¸°', 'ì´ì™„ê¸°'],
            'ì‹¬ì¥': ['cardiac', 'heart', 'ì‹¬ê·¼', 'ê´€ìƒë™ë§¥', 'ì‹¬í˜ˆê´€', 'ë¶€ì •ë§¥'],
            'ë‹¹ë‡¨': ['diabetes', 'í˜ˆë‹¹', 'ì¸ìŠë¦°', 'ë‹¹ë‡¨ë³‘', 'dm'],
            'í†µì¦': ['pain', 'ì•„í””', 'ache', 'ë¶ˆí¸ê°', 'ì••ë°•ê°'],
            'í˜¸í¡': ['respiratory', 'breathing', 'ìˆ¨', 'í˜¸í¡ê³¤ë€', 'ê¸°ì¹¨', 'í'],
            'í˜ˆë‹¹': ['glucose', 'sugar', 'ë‹¹ë‡¨', 'diabetes', 'blood sugar']
        }
        
        # ì¦ìƒ ê´€ë ¨ í™•ì¥
        symptom_expansions = {
            'ë‹¤ë‡¨': ['ë¹ˆë‡¨', 'ì†Œë³€', 'í™”ì¥ì‹¤', 'ì•¼ê°„ë‡¨'],
            'ë‹¤ê°ˆ': ['ê°ˆì¦', 'ëª©ë§ˆë¦„', 'ìˆ˜ë¶„'],
            'ì‹œì•¼': ['ì‹œë ¥', 'ëˆˆ', 'ë§ë§‰', 'ì‹œì•¼ì¥ì• '],
            'í”¼ë¡œ': ['ë¬´ë ¥ê°', 'ê¸°ë ¥', 'ì²´ë ¥', 'ë¬´ê¸°ë ¥'],
            'ì²´ì¤‘': ['ëª¸ë¬´ê²Œ', 'ì‚´', 'ë¹„ë§Œ', 'ì²´ì¤‘ê°ì†Œ', 'ì²´ì¤‘ì¦ê°€']
        }
        
        # ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì°¾ì•„ì„œ í™•ì¥
        query_lower = query.lower()
        for keyword, expansions in {**medical_expansions, **symptom_expansions}.items():
            if keyword in query_lower:
                expanded.extend(expansions)
        
        return list(set(expanded))  # ì¤‘ë³µ ì œê±°
    
    def _search_json_data(self, query: str, search_type: str) -> List[Dict]:
        """JSON ë°ì´í„°ì—ì„œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        from medical_tools import PatientSearchTool
        patient_search = PatientSearchTool()
        
        try:
            result = patient_search._run(query, query_type=search_type)
            result_data = json.loads(result)
            return result_data.get("patients", [])
        except:
            return []
    
    def _deduplicate_and_score(self, results: List[Dict], original_query: str) -> List[Dict]:
        """ê²°ê³¼ ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ì„± ìŠ¤ì½”ì–´ ê³„ì‚°"""
        seen_ids = set()
        unique_results = []
        
        for patient in results:
            patient_id = patient.get('id', '')
            if patient_id and patient_id not in seen_ids:
                seen_ids.add(patient_id)
                
                # ê´€ë ¨ì„± ìŠ¤ì½”ì–´ ê³„ì‚° (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
                score = self._calculate_relevance_score(patient, original_query)
                
                # ë²¡í„° ê²€ìƒ‰ í˜•íƒœë¡œ ë³€í™˜
                unique_results.append({
                    "content": self._patient_to_content(patient),
                    "metadata": {
                        "patient_id": patient_id,
                        "name": patient.get('name', ''),
                        "department": patient.get('department', ''),
                        "relevance_type": "keyword_match"
                    },
                    "similarity_score": score
                })
        
        # ìŠ¤ì½”ì–´ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        unique_results.sort(key=lambda x: x["similarity_score"], reverse=True)
        return unique_results
    
    def _calculate_relevance_score(self, patient: Dict, query: str) -> float:
        """í™˜ìì™€ ì¿¼ë¦¬ì˜ ê´€ë ¨ì„± ìŠ¤ì½”ì–´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        score = 0.0
        query_lower = query.lower()
        
        # ì§„ë‹¨ëª… ë§¤ì¹­ (ë†’ì€ ê°€ì¤‘ì¹˜)
        diagnoses = patient.get('diagnoses', [])
        for diag in diagnoses:
            if isinstance(diag, dict):
                diag_name = diag.get('name', '').lower()
                if query_lower in diag_name or any(word in diag_name for word in query_lower.split()):
                    score += 0.9
        
        # ì¦ìƒ ë§¤ì¹­
        symptoms = patient.get('symptoms', [])
        if isinstance(symptoms, list):
            for symptom in symptoms:
                if query_lower in symptom.lower():
                    score += 0.7
        
        # ì•½ë¬¼ ë§¤ì¹­
        medications = patient.get('medications', [])
        for med in medications:
            if isinstance(med, dict):
                med_name = med.get('medication', '').lower()
                if query_lower in med_name:
                    score += 0.5
        
        # ê¸°ë³¸ ìŠ¤ì½”ì–´ (0.1-1.0 ë²”ìœ„ë¡œ ì •ê·œí™”)
        return min(max(score, 0.1), 1.0)
    
    def _patient_to_content(self, patient: Dict) -> str:
        """í™˜ì ì •ë³´ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        content_parts = []
        
        # ê¸°ë³¸ ì •ë³´
        content_parts.append(f"í™˜ì: {patient.get('name', '')} ({patient.get('age', '')}ì„¸)")
        
        # ì§„ë‹¨ëª…
        diagnoses = patient.get('diagnoses', [])
        if diagnoses:
            diag_names = [d.get('name', '') for d in diagnoses if isinstance(d, dict)]
            content_parts.append(f"ì§„ë‹¨: {', '.join(diag_names)}")
        
        # ì¦ìƒ
        symptoms = patient.get('symptoms', [])
        if symptoms:
            symptoms_text = ', '.join(symptoms) if isinstance(symptoms, list) else str(symptoms)
            content_parts.append(f"ì¦ìƒ: {symptoms_text}")
        
        # ë¶€ì„œ
        if patient.get('department'):
            content_parts.append(f"ë¶€ì„œ: {patient['department']}")
        
        return ' | '.join(content_parts)


class SOAPNoteGeneratorTool(BaseTool):
    """SOAP ë…¸íŠ¸ ìƒì„± ë„êµ¬"""
    
    name: str = "soap_note_generator"
    description: str = "í™˜ì ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ SOAP ë…¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
    
    def _run(self, patient_data: str) -> str:
        """SOAP ë…¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            data = json.loads(patient_data)
            
            # SOAP ë…¸íŠ¸ êµ¬ì¡° ìƒì„±
            subjective = self._generate_subjective(data)
            objective = self._generate_objective(data)
            assessment = self._generate_assessment(data)
            plan = self._generate_plan(data)
            
            soap_note = {
                "record_id": data.get('record_id', f"SOAP_{datetime.now().strftime('%Y%m%d_%H%M%S')}"),
                "subjective": subjective,
                "objective": objective,
                "assessment": assessment,
                "plan": plan,
                "created_by": "AI Medical Agent",
                "created_at": datetime.now().isoformat()
            }
            
            return json.dumps(soap_note, ensure_ascii=False, indent=2)
            
        except Exception as e:
            return json.dumps({"error": f"SOAP note generation failed: {e}"})
    
    def _generate_subjective(self, data: Dict) -> str:
        """ì£¼ê´€ì  ì •ë³´ (S) ìƒì„±"""
        subjective_parts = []
        
        if 'chief_complaint' in data:
            subjective_parts.append(f"ì£¼ í˜¸ì†Œ: {data['chief_complaint']}")
        
        if 'symptoms' in data:
            symptoms_text = ", ".join(data['symptoms'])
            subjective_parts.append(f"ì¦ìƒ: {symptoms_text}")
        
        if 'pain_scale' in data:
            subjective_parts.append(f"í†µì¦ ì •ë„: {data['pain_scale']}/10")
        
        return "\n".join(subjective_parts)
    
    def _generate_objective(self, data: Dict) -> str:
        """ê°ê´€ì  ì •ë³´ (O) ìƒì„±"""
        objective_parts = []
        
        if 'vital_signs' in data:
            vital_signs = data['vital_signs']
            vital_text = []
            for key, value in vital_signs.items():
                vital_text.append(f"{key}: {value}")
            objective_parts.append(f"í™œë ¥ì§•í›„: {', '.join(vital_text)}")
        
        if 'physical_exam' in data:
            objective_parts.append(f"ì‹ ì²´ê²€ì‚¬: {data['physical_exam']}")
        
        if 'lab_results' in data:
            objective_parts.append(f"ê²€ì‚¬ê²°ê³¼: {data['lab_results']}")
        
        return "\n".join(objective_parts)
    
    def _generate_assessment(self, data: Dict) -> str:
        """í‰ê°€ (A) ìƒì„±"""
        assessment_parts = []
        
        if 'diagnosis' in data:
            if isinstance(data['diagnosis'], list):
                for i, diag in enumerate(data['diagnosis'], 1):
                    assessment_parts.append(f"{i}. {diag}")
            else:
                assessment_parts.append(f"1. {data['diagnosis']}")
        
        if 'differential_diagnosis' in data:
            assessment_parts.append(f"ê°ë³„ì§„ë‹¨: {data['differential_diagnosis']}")
        
        return "\n".join(assessment_parts)
    
    def _generate_plan(self, data: Dict) -> str:
        """ê³„íš (P) ìƒì„±"""
        plan_parts = []
        
        if 'medications' in data:
            plan_parts.append("ì²˜ë°©:")
            for med in data['medications']:
                if isinstance(med, dict):
                    med_text = f"- {med.get('name', '')} {med.get('dosage', '')} {med.get('frequency', '')}"
                else:
                    med_text = f"- {med}"
                plan_parts.append(med_text)
        
        if 'follow_up' in data:
            plan_parts.append(f"ì¶”í›„ ê´€ë¦¬: {data['follow_up']}")
        
        if 'patient_education' in data:
            plan_parts.append(f"í™˜ì êµìœ¡: {data['patient_education']}")
        
        return "\n".join(plan_parts)


class DrugInteractionCheckerTool(BaseTool):
    """ì•½ë¬¼ ìƒí˜¸ì‘ìš© ê²€ì‚¬ ë„êµ¬"""
    
    name: str = "drug_interaction_checker"
    description: str = "ì²˜ë°©ëœ ì•½ë¬¼ë“¤ ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ê²€ì‚¬í•©ë‹ˆë‹¤."
    interaction_db: Dict = Field(default_factory=dict)
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # ê°„ë‹¨í•œ ì•½ë¬¼ ìƒí˜¸ì‘ìš© ë°ì´í„°ë² ì´ìŠ¤ (ì‹¤ì œë¡œëŠ” ì™¸ë¶€ API ì‚¬ìš©)
        self.interaction_db = {
            ("ì•„ìŠ¤í”¼ë¦°", "ì™€íŒŒë¦°"): {
                "severity": "ë†’ìŒ",
                "description": "ì¶œí˜ˆ ìœ„í—˜ ì¦ê°€",
                "recommendation": "INR ëª¨ë‹ˆí„°ë§ ê°•í™”"
            },
            ("ë©”íŠ¸í¬ë¥´ë¯¼", "ìš”ì˜¤ë“œ ì¡°ì˜ì œ"): {
                "severity": "ë³´í†µ",
                "description": "ì –ì‚°ì¦ ìœ„í—˜",
                "recommendation": "ì¡°ì˜ì œ ì‚¬ìš© ì „í›„ ë©”íŠ¸í¬ë¥´ë¯¼ ì¤‘ë‹¨"
            }
        }
    
    def _run(self, medications: str) -> str:
        """ì•½ë¬¼ ìƒí˜¸ì‘ìš©ì„ ê²€ì‚¬í•©ë‹ˆë‹¤."""
        try:
            med_list = json.loads(medications)
            interactions = []
            
            for i in range(len(med_list)):
                for j in range(i + 1, len(med_list)):
                    drug1 = med_list[i].get('name', '') if isinstance(med_list[i], dict) else med_list[i]
                    drug2 = med_list[j].get('name', '') if isinstance(med_list[j], dict) else med_list[j]
                    
                    # ìƒí˜¸ì‘ìš© ê²€ì‚¬
                    interaction_key = (drug1, drug2)
                    reverse_key = (drug2, drug1)
                    
                    if interaction_key in self.interaction_db:
                        interaction_data = self.interaction_db[interaction_key]
                        interactions.append({
                            "drug1": drug1,
                            "drug2": drug2,
                            **interaction_data
                        })
                    elif reverse_key in self.interaction_db:
                        interaction_data = self.interaction_db[reverse_key]
                        interactions.append({
                            "drug1": drug2,
                            "drug2": drug1,
                            **interaction_data
                        })
            
            return json.dumps({
                "interactions": interactions,
                "total_interactions": len(interactions),
                "checked_medications": med_list
            }, ensure_ascii=False, indent=2)
            
        except Exception as e:
            return json.dumps({"error": f"Drug interaction check failed: {e}"})


class UrgencyAssessmentTool(BaseTool):
    """ì‘ê¸‰ë„ í‰ê°€ ë„êµ¬"""
    
    name: str = "urgency_assessment"
    description: str = "í™˜ìì˜ ì¦ìƒê³¼ í™œë ¥ì§•í›„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ê¸‰ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤."
    
    def _run(self, patient_data: str) -> str:
        """ì‘ê¸‰ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤."""
        try:
            data = json.loads(patient_data)
            urgency_score = 0
            factors = []
            
            # í™œë ¥ì§•í›„ í‰ê°€
            vital_signs = data.get('vital_signs', {})
            
            # í˜ˆì•• í‰ê°€
            if 'systolic_bp' in vital_signs:
                systolic = vital_signs['systolic_bp']
                if systolic > 180 or systolic < 90:
                    urgency_score += 3
                    factors.append("í˜ˆì•• ì´ìƒ")
            
            # ë§¥ë°• í‰ê°€
            if 'heart_rate' in vital_signs:
                hr = vital_signs['heart_rate']
                if hr > 120 or hr < 50:
                    urgency_score += 2
                    factors.append("ë§¥ë°• ì´ìƒ")
            
            # ì²´ì˜¨ í‰ê°€
            if 'temperature' in vital_signs:
                temp = vital_signs['temperature']
                if temp > 39.0 or temp < 35.0:
                    urgency_score += 2
                    factors.append("ì²´ì˜¨ ì´ìƒ")
            
            # ì¦ìƒ í‰ê°€
            symptoms = data.get('symptoms', [])
            critical_symptoms = ['í‰í†µ', 'í˜¸í¡ê³¤ë€', 'ì˜ì‹ì €í•˜', 'ì‹¬í•œë³µí†µ', 'ì¶œí˜ˆ']
            
            for symptom in symptoms:
                if any(critical in symptom for critical in critical_symptoms):
                    urgency_score += 3
                    factors.append(f"ìœ„í—˜ ì¦ìƒ: {symptom}")
            
            # ì‘ê¸‰ë„ ê²°ì •
            if urgency_score >= 7:
                urgency_level = UrgencyLevel.CRITICAL
            elif urgency_score >= 5:
                urgency_level = UrgencyLevel.HIGH
            elif urgency_score >= 3:
                urgency_level = UrgencyLevel.MEDIUM
            else:
                urgency_level = UrgencyLevel.LOW
            
            return json.dumps({
                "urgency_level": urgency_level.value,
                "urgency_score": urgency_score,
                "contributing_factors": factors,
                "recommendations": self._get_urgency_recommendations(urgency_level)
            }, ensure_ascii=False, indent=2)
            
        except Exception as e:
            return json.dumps({"error": f"Urgency assessment failed: {e}"})
    
    def _get_urgency_recommendations(self, urgency_level: UrgencyLevel) -> List[str]:
        """ì‘ê¸‰ë„ë³„ ê¶Œì¥ì‚¬í•­ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        recommendations = {
            UrgencyLevel.CRITICAL: [
                "ì¦‰ì‹œ ì‘ê¸‰ì‹¤ ì´ì†¡",
                "í™œë ¥ì§•í›„ ì§€ì† ëª¨ë‹ˆí„°ë§",
                "ì‘ê¸‰ì²˜ì¹˜ ì¤€ë¹„"
            ],
            UrgencyLevel.HIGH: [
                "ìš°ì„  ì§„ë£Œ í•„ìš”",
                "30ë¶„ ì´ë‚´ ì¬í‰ê°€",
                "ì „ë¬¸ì˜ ìƒë‹´"
            ],
            UrgencyLevel.MEDIUM: [
                "1ì‹œê°„ ì´ë‚´ ì§„ë£Œ",
                "ì¦ìƒ ë³€í™” ê´€ì°°",
                "í•„ìš”ì‹œ ì¬í‰ê°€"
            ],
            UrgencyLevel.LOW: [
                "ì¼ë°˜ ì§„ë£Œ ëŒ€ê¸°",
                "ì¦ìƒ ì•…í™”ì‹œ ì¬í‰ê°€",
                "í™˜ì êµìœ¡ ì œê³µ"
            ]
        }
        
        return recommendations.get(urgency_level, []) 


class HybridSearchTool(BaseTool):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë„êµ¬ - JSON ë°ì´í„°ì™€ ë²¡í„° ê²€ìƒ‰ ê²°í•©"""
    
    name: str = "hybrid_search"
    description: str = "JSON ë°ì´í„°ì™€ ë²¡í„° ê²€ìƒ‰ì„ ê²°í•©í•˜ì—¬ í¬ê´„ì ì¸ í™˜ì ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
    data_path: str = Field(default="")
    openai_api_key: str = Field(default="")
    gemini_api_key: str = Field(default="")
    patient_search: Optional[PatientSearchTool] = Field(default=None)
    vector_search: Optional[VectorSearchTool] = Field(default=None)
    
    def __init__(self, data_path: str, openai_api_key: str = "", gemini_api_key: str = "", **kwargs):
        super().__init__(**kwargs)
        self.data_path = data_path
        self.openai_api_key = openai_api_key
        self.gemini_api_key = gemini_api_key
        self.patient_search = PatientSearchTool(data_path=data_path)
        self.vector_search = VectorSearchTool(
            openai_api_key=openai_api_key,
            gemini_api_key=gemini_api_key
        )
    
    def _run(self, query: str, search_type: str = "comprehensive") -> str:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            results = {
                "query": query,
                "search_type": search_type,
                "json_results": {},
                "vector_results": {},
                "combined_insights": []
            }
            
            # 1. JSON ë°ì´í„° ê²€ìƒ‰
            # ì¿¼ë¦¬ í˜•íƒœì— ë”°ë¼ ì ì ˆí•œ ê²€ìƒ‰ ìˆ˜í–‰
            if ":" not in query:
                # ë‹¨ìˆœ í‚¤ì›Œë“œì¸ ê²½ìš° ì§„ë‹¨ëª…ìœ¼ë¡œ ê²€ìƒ‰
                search_query = f"ì§„ë‹¨: {query}"
            else:
                search_query = query
            
            json_result = self.patient_search._run(search_query)
            results["json_results"] = json.loads(json_result)
            
            # 2. ë²¡í„° ê²€ìƒ‰ (ì¦ìƒ ê¸°ë°˜)
            vector_result = self.vector_search._run(query)
            results["vector_results"] = json.loads(vector_result)
            
            # 3. ê²°ê³¼ ê²°í•© ë° ì¸ì‚¬ì´íŠ¸ ìƒì„±
            insights = []
            
            if results["json_results"]["total_count"] > 0:
                insights.append(f"êµ¬ì¡°í™”ëœ ë°ì´í„°ì—ì„œ {results['json_results']['total_count']}ëª…ì˜ í™˜ìë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            
            if results["vector_results"].get("total_found", 0) > 0:
                insights.append(f"ìœ ì‚¬ ì¦ìƒ ê²€ìƒ‰ì—ì„œ {results['vector_results']['total_found']}ê°œì˜ ê´€ë ¨ ì‚¬ë¡€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            
            if not insights:
                insights.append("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
            
            results["combined_insights"] = insights
            
            return json.dumps(results, ensure_ascii=False, indent=2)
            
        except Exception as e:
            return json.dumps({"error": f"Hybrid search failed: {e}"})


class MCPConnectorTool(BaseTool):
    """MCP(Model Context Protocol) ì—°ê²° ë„êµ¬"""
    
    name: str = "mcp_connector"
    description: str = "ì™¸ë¶€ ì˜ë£Œ ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ì‹œìŠ¤í…œê³¼ MCPë¥¼ í†µí•´ ì—°ê²°í•©ë‹ˆë‹¤."
    mcp_endpoints: Dict[str, str] = Field(default_factory=dict)
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # ì‹¤ì œ MCP ì„œë²„ ì—”ë“œí¬ì¸íŠ¸ë¡œ ìˆ˜ì •
        self.mcp_endpoints = {
            "pubmed": "http://localhost:8080",
            "memory": "http://localhost:8081", 
            "hospital_db": "http://localhost:8080",
            "medical_records": "http://localhost:8081"
        }
    
    async def _call_mcp_async(self, endpoint_url: str, query: str) -> Dict[str, Any]:
        """ë¹„ë™ê¸° MCP í˜¸ì¶œ"""
        try:
            # MCP JSON-RPC ìš”ì²­ í¬ë§·
            mcp_request = {
                "jsonrpc": "2.0",
                "method": "search",
                "params": {
                    "query": query,
                    "context": "medical_data"
                },
                "id": 1
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    endpoint_url,
                    json=mcp_request,
                    headers={"Content-Type": "application/json"},
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result
                    else:
                        return {
                            "error": f"HTTP {response.status}: {await response.text()}"
                        }
        except Exception as e:
            return {"error": f"MCP ì—°ê²° ì‹¤íŒ¨: {str(e)}"}
    
    def _call_mcp_sync(self, endpoint_url: str, query: str) -> Dict[str, Any]:
        """ë™ê¸° MCP í˜¸ì¶œ - ì‹¤ì œ MCP ì„œë²„ API í˜•ì‹"""
        try:
            # PubMed ì„œë²„ì¸ ê²½ìš°
            if "8080" in endpoint_url:
                mcp_request = {
                    "parameters": {
                        "query": query,
                        "max_results": 5
                    }
                }
                response = requests.post(
                    f"{endpoint_url}/tools/search_pubmed",
                    json=mcp_request,
                    headers={"Content-Type": "application/json"},
                    timeout=30
                )
            # Memory ì„œë²„ì¸ ê²½ìš°  
            elif "8081" in endpoint_url:
                mcp_request = {
                    "parameters": {
                        "session_id": "agent_session",
                        "content": f"Agent searched for: {query}",
                        "entry_type": "agent_search"
                    }
                }
                response = requests.post(
                    f"{endpoint_url}/tools/save_memory",
                    json=mcp_request,
                    headers={"Content-Type": "application/json"},
                    timeout=30
                )
            else:
                return {"error": f"Unknown MCP server: {endpoint_url}"}
            
            if response.status_code == 200:
                return {
                    "success": True,
                    "mcp_result": response.json(),
                    "endpoint": endpoint_url
                }
            else:
                return {
                    "error": f"HTTP {response.status_code}: {response.text}"
                }
        except Exception as e:
            return {"error": f"MCP ì—°ê²° ì‹¤íŒ¨: {str(e)}"}
    
    def _run(self, query: str, endpoint: str = "hospital_db") -> str:
        """MCPë¥¼ í†µí•´ ì™¸ë¶€ ì‹œìŠ¤í…œì— ì¿¼ë¦¬ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤."""
        try:
            if endpoint not in self.mcp_endpoints:
                return json.dumps({"error": f"Unknown endpoint: {endpoint}"})
            
            endpoint_url = self.mcp_endpoints[endpoint]
            
            # ì‹¤ì œ MCP ì„œë²„ê°€ ë™ì‘ì¤‘ì¸ì§€ í™•ì¸í•˜ê³  í˜¸ì¶œ
            try:
                # ë¨¼ì € pingì„ í†µí•´ ì„œë²„ ìƒíƒœ í™•ì¸
                ping_response = requests.get(f"{endpoint_url}/health", timeout=5)
                if ping_response.status_code == 200:
                    # ì‹¤ì œ MCP ì„œë²„ê°€ ìˆìœ¼ë©´ í˜¸ì¶œ
                    mcp_response = self._call_mcp_sync(endpoint_url, query)
                else:
                    # ì„œë²„ê°€ ì—†ìœ¼ë©´ ì‹œë®¬ë ˆì´ì…˜ ì‘ë‹µ
                    mcp_response = self._create_simulation_response(endpoint, query)
            except (requests.ConnectionError, requests.Timeout):
                # ì—°ê²° ì‹¤íŒ¨ì‹œ ì‹œë®¬ë ˆì´ì…˜ ì‘ë‹µ
                mcp_response = self._create_simulation_response(endpoint, query)
            
            return json.dumps(mcp_response, ensure_ascii=False, indent=2)
            
        except Exception as e:
            return json.dumps({"error": f"MCP connection failed: {e}"})
    
    def _create_simulation_response(self, endpoint: str, query: str) -> Dict[str, Any]:
        """MCP ì„œë²„ê°€ ì—†ì„ ë•Œ ì‹œë®¬ë ˆì´ì…˜ ì‘ë‹µ ìƒì„±"""
        return {
            "jsonrpc": "2.0",
            "result": {
                "endpoint": endpoint,
                "query": query,
                "status": "simulated",
                "data": {
                    "message": f"MCP ì‹œë®¬ë ˆì´ì…˜: {endpoint}ì—ì„œ '{query}' ê²€ìƒ‰ ì™„ë£Œ",
                    "external_results": [
                        {
                            "source": endpoint,
                            "content": f"{query}ì™€ ê´€ë ¨ëœ ì™¸ë¶€ ì˜ë£Œ ë°ì´í„° (ì‹œë®¬ë ˆì´ì…˜)",
                            "confidence": 0.8,
                            "type": "simulation"
                        }
                    ]
                },
                "timestamp": datetime.now().isoformat(),
                "simulation": True
            },
            "id": 1
        } 